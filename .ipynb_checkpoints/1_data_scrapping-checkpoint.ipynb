{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dfad5d-42bf-4c45-a7fa-d44bd10b356b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8821383d-1091-4843-b2fb-c04fe11845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the beautifulsoup \n",
    "# and request libraries of python.\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd46d7d-f511-40ea-b335-4d00c948cdb0",
   "metadata": {},
   "source": [
    "# Scrapping Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb198e5a-5b45-41fa-9eb4-b19d42ea79b6",
   "metadata": {},
   "source": [
    "We will scrap a search result from google for 500 content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3957c750-03fe-44b9-b10b-a83b08b614d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping for page 1, request status: <Response [200]> ...\n",
      "scrapping for page 2, request status: <Response [200]> ...\n",
      "scrapping for page 3, request status: <Response [200]> ...\n",
      "scrapping for page 4, request status: <Response [200]> ...\n",
      "scrapping for page 5, request status: <Response [200]> ...\n",
      "scrapping for page 6, request status: <Response [200]> ...\n",
      "scrapping for page 7, request status: <Response [200]> ...\n",
      "scrapping for page 8, request status: <Response [200]> ...\n",
      "scrapping for page 9, request status: <Response [200]> ...\n",
      "scrapping for page 10, request status: <Response [200]> ...\n",
      "scrapping for page 11, request status: <Response [200]> ...\n",
      "scrapping for page 12, request status: <Response [200]> ...\n",
      "scrapping for page 13, request status: <Response [200]> ...\n",
      "scrapping for page 14, request status: <Response [200]> ...\n",
      "scrapping for page 15, request status: <Response [200]> ...\n",
      "scrapping for page 16, request status: <Response [200]> ...\n",
      "scrapping for page 17, request status: <Response [200]> ...\n",
      "scrapping for page 18, request status: <Response [200]> ...\n",
      "scrapping for page 19, request status: <Response [200]> ...\n",
      "scrapping for page 20, request status: <Response [200]> ...\n",
      "scrapping for page 21, request status: <Response [200]> ...\n",
      "scrapping for page 22, request status: <Response [200]> ...\n",
      "scrapping for page 23, request status: <Response [200]> ...\n",
      "scrapping for page 24, request status: <Response [200]> ...\n",
      "scrapping for page 25, request status: <Response [200]> ...\n",
      "scrapping for page 26, request status: <Response [200]> ...\n",
      "scrapping for page 27, request status: <Response [200]> ...\n",
      "scrapping for page 28, request status: <Response [200]> ...\n",
      "scrapping for page 29, request status: <Response [200]> ...\n",
      "scrapping for page 30, request status: <Response [200]> ...\n",
      "scrapping for page 31, request status: <Response [200]> ...\n",
      "scrapping for page 32, request status: <Response [200]> ...\n",
      "scrapping for page 33, request status: <Response [200]> ...\n",
      "scrapping for page 34, request status: <Response [200]> ...\n",
      "scrapping for page 35, request status: <Response [200]> ...\n",
      "scrapping for page 36, request status: <Response [200]> ...\n",
      "scrapping for page 37, request status: <Response [200]> ...\n",
      "scrapping for page 38, request status: <Response [200]> ...\n",
      "scrapping for page 39, request status: <Response [200]> ...\n",
      "scrapping for page 40, request status: <Response [200]> ...\n",
      "scrapping for page 41, request status: <Response [200]> ...\n",
      "scrapping for page 42, request status: <Response [200]> ...\n",
      "scrapping for page 43, request status: <Response [200]> ...\n",
      "scrapping for page 44, request status: <Response [200]> ...\n",
      "scrapping for page 45, request status: <Response [200]> ...\n",
      "scrapping for page 46, request status: <Response [200]> ...\n",
      "scrapping for page 47, request status: <Response [200]> ...\n",
      "scrapping for page 48, request status: <Response [200]> ...\n",
      "scrapping for page 49, request status: <Response [200]> ...\n",
      "scrapping for page 50, request status: <Response [200]> ...\n",
      "\n",
      "execution time completed: 0:06:10.638039\n"
     ]
    }
   ],
   "source": [
    "input_text = 'linux'\n",
    "search_text = input_text.replace(' ','+')\n",
    "\n",
    "# define content_id\n",
    "n_content = 500\n",
    "# n content in google search per page is 10 contents\n",
    "content_ids = np.arange(0, n_content, 10)\n",
    "\n",
    "list_soup = []\n",
    "\n",
    "# get start time\n",
    "start_time = datetime.now()\n",
    "for i, start_id in enumerate(content_ids):\n",
    "    url = f'https://www.google.com/search?q={search_text}&start={start_id}'\n",
    "\n",
    "    # Fetch the URL data\n",
    "    request_result=requests.get( url )\n",
    "    \n",
    "    print(f'scrapping for page {i+1}, request status: {request_result} ...')\n",
    "\n",
    "    # Creating soup from the fetched request\n",
    "    soup = bs4.BeautifulSoup(request_result.text,\n",
    "                             \"html.parser\")\n",
    "    \n",
    "    # append result per peage\n",
    "    list_soup.append(soup)\n",
    "    \n",
    "    # pause search for evry iteration with random number\n",
    "    # the more number the more delay\n",
    "    # it is usefull to avoid security issue\n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "# get finish time\n",
    "finish_time = datetime.now()\n",
    "print('\\nexecution time completed:', finish_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652940a-c818-4963-a819-e092db0fb727",
   "metadata": {},
   "source": [
    "all responses code are 200, so the get request is complete for all pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cb810-97d0-41fc-8f4e-01b50bac24f9",
   "metadata": {},
   "source": [
    "# Dataframe from Scrapping Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c1a588-2fe4-49cf-8eb8-f940f8500d8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "page_list = []\n",
    "rank_list = []\n",
    "\n",
    "# initial page\n",
    "web_page = 1\n",
    "rank_item = 1\n",
    "\n",
    "# loop for all result\n",
    "for i, soup in enumerate(list_soup):\n",
    "\n",
    "    # get links in website page\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    # get url and website title\n",
    "    for link in links:\n",
    "        link_href = link.get('href')\n",
    "        \n",
    "        if (\"url?q=\" in link_href) and (not \"webcache\" in link_href):\n",
    "            # get content title\n",
    "            title = link.find_all('h3')\n",
    "            \n",
    "            if len(title) > 0:\n",
    "                url_web = link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]\n",
    "                title_web = title[0].getText()\n",
    "                \n",
    "                title_list.extend([title_web])\n",
    "                url_list.extend([url_web])\n",
    "                page_list.extend([web_page])\n",
    "                rank_list.extend([rank_item])\n",
    "                \n",
    "                rank_item+=1\n",
    "    web_page+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354e59c4-6d7a-45f4-aca7-1b614d0324bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page number</th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>www.linux.org</td>\n",
       "      <td>https://www.linux.org/</td>\n",
       "      <td>Linux.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>id.wikipedia.org</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Linux</td>\n",
       "      <td>Linux - Wikipedia bahasa Indonesia, ensikloped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Linux</td>\n",
       "      <td>Linux - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>tekno.kompas.com</td>\n",
       "      <td>https://tekno.kompas.com/read/2022/07/27/12150...</td>\n",
       "      <td>Apa Itu Linux? Mengenal Fungsi, Sejarah, serta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>diskominfo.kedirikab.go.id</td>\n",
       "      <td>https://diskominfo.kedirikab.go.id/baca/apa-it...</td>\n",
       "      <td>Apa itu Linux? - Kominfo Kab Kediri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>22</td>\n",
       "      <td>204</td>\n",
       "      <td>lms.onnocenter.or.id</td>\n",
       "      <td>https://lms.onnocenter.or.id/wiki/index.php/OS...</td>\n",
       "      <td>OS: Linux Buatan Indonesia - OnnoWiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>22</td>\n",
       "      <td>205</td>\n",
       "      <td>www.techlila.com</td>\n",
       "      <td>https://www.techlila.com/id/introduction-linux...</td>\n",
       "      <td>Pengenalan Sistem Operasi Linux - TechLila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>22</td>\n",
       "      <td>206</td>\n",
       "      <td>sevenmediatech.co.id</td>\n",
       "      <td>https://sevenmediatech.co.id/blog/view/web-dev...</td>\n",
       "      <td>Web Developer Pilih Windows atau Linux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>22</td>\n",
       "      <td>207</td>\n",
       "      <td>mahasiswa.ung.ac.id</td>\n",
       "      <td>https://mahasiswa.ung.ac.id/532413025/home/201...</td>\n",
       "      <td>PROGRAM IT TENTANG SISTEM OPERASI LINUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>22</td>\n",
       "      <td>208</td>\n",
       "      <td>www.opensourceforu.com</td>\n",
       "      <td>https://www.opensourceforu.com/2020/03/reasons...</td>\n",
       "      <td>Ten reasons why we should use Linux - Open Sou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page number  rank                      domain  \\\n",
       "0              1     1               www.linux.org   \n",
       "1              1     2            id.wikipedia.org   \n",
       "2              1     3            en.wikipedia.org   \n",
       "3              1     4            tekno.kompas.com   \n",
       "4              1     5  diskominfo.kedirikab.go.id   \n",
       "..           ...   ...                         ...   \n",
       "203           22   204        lms.onnocenter.or.id   \n",
       "204           22   205            www.techlila.com   \n",
       "205           22   206        sevenmediatech.co.id   \n",
       "206           22   207         mahasiswa.ung.ac.id   \n",
       "207           22   208      www.opensourceforu.com   \n",
       "\n",
       "                                                   url  \\\n",
       "0                               https://www.linux.org/   \n",
       "1                  https://id.wikipedia.org/wiki/Linux   \n",
       "2                  https://en.wikipedia.org/wiki/Linux   \n",
       "3    https://tekno.kompas.com/read/2022/07/27/12150...   \n",
       "4    https://diskominfo.kedirikab.go.id/baca/apa-it...   \n",
       "..                                                 ...   \n",
       "203  https://lms.onnocenter.or.id/wiki/index.php/OS...   \n",
       "204  https://www.techlila.com/id/introduction-linux...   \n",
       "205  https://sevenmediatech.co.id/blog/view/web-dev...   \n",
       "206  https://mahasiswa.ung.ac.id/532413025/home/201...   \n",
       "207  https://www.opensourceforu.com/2020/03/reasons...   \n",
       "\n",
       "                                                 title  \n",
       "0                                            Linux.org  \n",
       "1    Linux - Wikipedia bahasa Indonesia, ensikloped...  \n",
       "2                                    Linux - Wikipedia  \n",
       "3    Apa Itu Linux? Mengenal Fungsi, Sejarah, serta...  \n",
       "4                  Apa itu Linux? - Kominfo Kab Kediri  \n",
       "..                                                 ...  \n",
       "203              OS: Linux Buatan Indonesia - OnnoWiki  \n",
       "204         Pengenalan Sistem Operasi Linux - TechLila  \n",
       "205             Web Developer Pilih Windows atau Linux  \n",
       "206            PROGRAM IT TENTANG SISTEM OPERASI LINUX  \n",
       "207  Ten reasons why we should use Linux - Open Sou...  \n",
       "\n",
       "[208 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create dataframe\n",
    "df_result = pd.DataFrame({'page number':page_list,\n",
    "                   'rank':rank_list,\n",
    "                   'url':url_list,\n",
    "                   \"title\":title_list})\n",
    "\n",
    "# insert url doman\n",
    "df_result.insert(loc=df_result.columns.get_loc('url'), column='domain', \n",
    "                 value=df_result['url'].str.split('/').str[2])\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcafe2db-c445-4d0c-8bf6-c5e5d5ac28f2",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab55eedd-9827-45d2-b147-60be64628a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "file_path = os.path.join('dataset', \"scrapping_result.csv\")\n",
    "df_result.to_csv(file_path, sep=',', index=False, quoting=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7a8bbf8280d739911ee0fb92c2a18e877feb5454e3bd0217f9b485bdc87c128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
