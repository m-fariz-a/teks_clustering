{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dfad5d-42bf-4c45-a7fa-d44bd10b356b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8821383d-1091-4843-b2fb-c04fe11845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the beautifulsoup \n",
    "# and request libraries of python.\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# functions which is special for this notebook\n",
    "from string_operation import StringOperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd46d7d-f511-40ea-b335-4d00c948cdb0",
   "metadata": {},
   "source": [
    "# Scrapping Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb198e5a-5b45-41fa-9eb4-b19d42ea79b6",
   "metadata": {},
   "source": [
    "We will scrap a search result from google for 500 content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3957c750-03fe-44b9-b10b-a83b08b614d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping for page 1, request status: <Response [200]> ...\n",
      "scrapping for page 2, request status: <Response [200]> ...\n",
      "scrapping for page 3, request status: <Response [200]> ...\n",
      "scrapping for page 4, request status: <Response [200]> ...\n",
      "scrapping for page 5, request status: <Response [200]> ...\n",
      "scrapping for page 6, request status: <Response [200]> ...\n",
      "scrapping for page 7, request status: <Response [200]> ...\n",
      "scrapping for page 8, request status: <Response [200]> ...\n",
      "scrapping for page 9, request status: <Response [200]> ...\n",
      "scrapping for page 10, request status: <Response [200]> ...\n",
      "scrapping for page 11, request status: <Response [200]> ...\n",
      "scrapping for page 12, request status: <Response [200]> ...\n",
      "scrapping for page 13, request status: <Response [200]> ...\n",
      "scrapping for page 14, request status: <Response [200]> ...\n",
      "scrapping for page 15, request status: <Response [200]> ...\n",
      "scrapping for page 16, request status: <Response [200]> ...\n",
      "scrapping for page 17, request status: <Response [200]> ...\n",
      "scrapping for page 18, request status: <Response [200]> ...\n",
      "scrapping for page 19, request status: <Response [200]> ...\n",
      "scrapping for page 20, request status: <Response [200]> ...\n",
      "scrapping for page 21, request status: <Response [200]> ...\n",
      "scrapping for page 22, request status: <Response [200]> ...\n",
      "scrapping for page 23, request status: <Response [200]> ...\n",
      "scrapping for page 24, request status: <Response [200]> ...\n",
      "scrapping for page 25, request status: <Response [200]> ...\n",
      "scrapping for page 26, request status: <Response [200]> ...\n",
      "scrapping for page 27, request status: <Response [200]> ...\n",
      "scrapping for page 28, request status: <Response [200]> ...\n",
      "scrapping for page 29, request status: <Response [200]> ...\n",
      "scrapping for page 30, request status: <Response [200]> ...\n",
      "scrapping for page 31, request status: <Response [200]> ...\n",
      "scrapping for page 32, request status: <Response [200]> ...\n",
      "scrapping for page 33, request status: <Response [200]> ...\n",
      "scrapping for page 34, request status: <Response [200]> ...\n",
      "scrapping for page 35, request status: <Response [200]> ...\n",
      "scrapping for page 36, request status: <Response [200]> ...\n",
      "scrapping for page 37, request status: <Response [200]> ...\n",
      "scrapping for page 38, request status: <Response [200]> ...\n",
      "scrapping for page 39, request status: <Response [200]> ...\n",
      "scrapping for page 40, request status: <Response [200]> ...\n",
      "scrapping for page 41, request status: <Response [200]> ...\n",
      "scrapping for page 42, request status: <Response [200]> ...\n",
      "scrapping for page 43, request status: <Response [200]> ...\n",
      "scrapping for page 44, request status: <Response [200]> ...\n",
      "scrapping for page 45, request status: <Response [200]> ...\n",
      "scrapping for page 46, request status: <Response [200]> ...\n",
      "scrapping for page 47, request status: <Response [200]> ...\n",
      "scrapping for page 48, request status: <Response [200]> ...\n",
      "scrapping for page 49, request status: <Response [200]> ...\n",
      "scrapping for page 50, request status: <Response [200]> ...\n",
      "\n",
      "execution time completed: 0:05:49.613646\n"
     ]
    }
   ],
   "source": [
    "input_text = 'linux'\n",
    "search_text = input_text.replace(' ','+')\n",
    "\n",
    "# define content_id\n",
    "n_content = 500\n",
    "# n content in google search per page is 10 contents\n",
    "content_ids = np.arange(0, n_content, 10)\n",
    "\n",
    "list_soup = []\n",
    "\n",
    "# get start time\n",
    "start_time = datetime.now()\n",
    "for i, start_id in enumerate(content_ids):\n",
    "    url = f'https://www.google.com/search?q={search_text}&start={start_id}'\n",
    "\n",
    "    # Fetch the URL data\n",
    "    request_result=requests.get( url )\n",
    "    \n",
    "    print(f'scrapping for page {i+1}, request status: {request_result} ...')\n",
    "\n",
    "    # Creating soup from the fetched request\n",
    "    soup = bs4.BeautifulSoup(request_result.text,\n",
    "                             \"html.parser\")\n",
    "    \n",
    "    # append result per peage\n",
    "    list_soup.append(soup)\n",
    "    \n",
    "    # pause search for evry iteration with random number\n",
    "    # the more number the more delay\n",
    "    # it is usefull to avoid security issue\n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "# get finish time\n",
    "finish_time = datetime.now()\n",
    "print('\\nexecution time completed:', finish_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652940a-c818-4963-a819-e092db0fb727",
   "metadata": {},
   "source": [
    "all responses code are 200, so the get request is complete for all pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cb810-97d0-41fc-8f4e-01b50bac24f9",
   "metadata": {},
   "source": [
    "# Dataframe from Scrapping Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c1a588-2fe4-49cf-8eb8-f940f8500d8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "page_list = []\n",
    "rank_list = []\n",
    "\n",
    "# initial page\n",
    "web_page = 1\n",
    "rank_item = 1\n",
    "\n",
    "# loop for all result\n",
    "for i, soup in enumerate(list_soup):\n",
    "\n",
    "    # get links in website page\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    # get url and website title\n",
    "    for link in links:\n",
    "        link_href = link.get('href')\n",
    "        \n",
    "        if (\"url?q=\" in link_href) and (not \"webcache\" in link_href):\n",
    "            # get content title\n",
    "            title = link.find_all('h3')\n",
    "            \n",
    "            if len(title) > 0:\n",
    "                url_web = link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]\n",
    "                title_web = title[0].getText()\n",
    "                \n",
    "                title_list.extend([title_web])\n",
    "                url_list.extend([url_web])\n",
    "                page_list.extend([web_page])\n",
    "                rank_list.extend([rank_item])\n",
    "                \n",
    "                rank_item+=1\n",
    "    web_page+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354e59c4-6d7a-45f4-aca7-1b614d0324bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page number</th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>original title</th>\n",
       "      <th>english title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>www.linux.org</td>\n",
       "      <td>https://www.linux.org/</td>\n",
       "      <td>Linux.org</td>\n",
       "      <td>Linux.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>id.wikipedia.org</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Linux</td>\n",
       "      <td>Linux - Wikipedia bahasa Indonesia, ensikloped...</td>\n",
       "      <td>Linux - Wikipedia Indonesian, the free encyclo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>tekno.kompas.com</td>\n",
       "      <td>https://tekno.kompas.com/read/2022/07/27/12150...</td>\n",
       "      <td>Apa Itu Linux? Mengenal Fungsi, Sejarah, serta...</td>\n",
       "      <td>What Is Linux? Get to know the Functions, Hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>diskominfo.kedirikab.go.id</td>\n",
       "      <td>https://diskominfo.kedirikab.go.id/baca/apa-it...</td>\n",
       "      <td>Apa itu Linux? - Kominfo Kab Kediri</td>\n",
       "      <td>What is Linux? - Kominfo Kediri District</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ubuntu.com</td>\n",
       "      <td>https://ubuntu.com/</td>\n",
       "      <td>Ubuntu: Enterprise Open Source and Linux</td>\n",
       "      <td>Ubuntu: Enterprise Open Source and Linux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>22</td>\n",
       "      <td>213</td>\n",
       "      <td>www.filemagz.com</td>\n",
       "      <td>https://www.filemagz.com/os-linux-terbaik/</td>\n",
       "      <td>OS Linux Terbaik untuk Kamu Coba, Apa Saja Ya?...</td>\n",
       "      <td>The Best Linux OS for You to Try, What Are The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>23</td>\n",
       "      <td>214</td>\n",
       "      <td>commons.wikimedia.org</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Linux_...</td>\n",
       "      <td>File:Linux Mint 20.3 (Una) Cinnamon.png - Wiki...</td>\n",
       "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>23</td>\n",
       "      <td>215</td>\n",
       "      <td>www.linuxadictos.com</td>\n",
       "      <td>https://www.linuxadictos.com/en/linux-mint-20-...</td>\n",
       "      <td>Linux Mint 20.3 now available, with Linux 5.4 ...</td>\n",
       "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>23</td>\n",
       "      <td>216</td>\n",
       "      <td>www.plex.tv</td>\n",
       "      <td>https://www.plex.tv/media-server-downloads/</td>\n",
       "      <td>Media Server Downloads - Plex</td>\n",
       "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>23</td>\n",
       "      <td>217</td>\n",
       "      <td>www.filemagz.com</td>\n",
       "      <td>https://www.filemagz.com/os-linux-terbaik/</td>\n",
       "      <td>OS Linux Terbaik untuk Kamu Coba, Apa Saja Ya?...</td>\n",
       "      <td>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page number  rank                      domain  \\\n",
       "0              1     1               www.linux.org   \n",
       "1              1     2            id.wikipedia.org   \n",
       "2              1     3            tekno.kompas.com   \n",
       "3              1     4  diskominfo.kedirikab.go.id   \n",
       "4              1     5                  ubuntu.com   \n",
       "..           ...   ...                         ...   \n",
       "212           22   213            www.filemagz.com   \n",
       "213           23   214       commons.wikimedia.org   \n",
       "214           23   215        www.linuxadictos.com   \n",
       "215           23   216                 www.plex.tv   \n",
       "216           23   217            www.filemagz.com   \n",
       "\n",
       "                                                   url  \\\n",
       "0                               https://www.linux.org/   \n",
       "1                  https://id.wikipedia.org/wiki/Linux   \n",
       "2    https://tekno.kompas.com/read/2022/07/27/12150...   \n",
       "3    https://diskominfo.kedirikab.go.id/baca/apa-it...   \n",
       "4                                  https://ubuntu.com/   \n",
       "..                                                 ...   \n",
       "212         https://www.filemagz.com/os-linux-terbaik/   \n",
       "213  https://commons.wikimedia.org/wiki/File:Linux_...   \n",
       "214  https://www.linuxadictos.com/en/linux-mint-20-...   \n",
       "215        https://www.plex.tv/media-server-downloads/   \n",
       "216         https://www.filemagz.com/os-linux-terbaik/   \n",
       "\n",
       "                                        original title  \\\n",
       "0                                            Linux.org   \n",
       "1    Linux - Wikipedia bahasa Indonesia, ensikloped...   \n",
       "2    Apa Itu Linux? Mengenal Fungsi, Sejarah, serta...   \n",
       "3                  Apa itu Linux? - Kominfo Kab Kediri   \n",
       "4             Ubuntu: Enterprise Open Source and Linux   \n",
       "..                                                 ...   \n",
       "212  OS Linux Terbaik untuk Kamu Coba, Apa Saja Ya?...   \n",
       "213  File:Linux Mint 20.3 (Una) Cinnamon.png - Wiki...   \n",
       "214  Linux Mint 20.3 now available, with Linux 5.4 ...   \n",
       "215                      Media Server Downloads - Plex   \n",
       "216  OS Linux Terbaik untuk Kamu Coba, Apa Saja Ya?...   \n",
       "\n",
       "                                         english title  \n",
       "0                                            Linux.org  \n",
       "1    Linux - Wikipedia Indonesian, the free encyclo...  \n",
       "2    What Is Linux? Get to know the Functions, Hist...  \n",
       "3             What is Linux? - Kominfo Kediri District  \n",
       "4             Ubuntu: Enterprise Open Source and Linux  \n",
       "..                                                 ...  \n",
       "212  The Best Linux OS for You to Try, What Are The...  \n",
       "213  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...  \n",
       "214  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...  \n",
       "215  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...  \n",
       "216  MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE ...  \n",
       "\n",
       "[217 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create dataframe\n",
    "df_result = pd.DataFrame({'page number':page_list,\n",
    "                   'rank':rank_list,\n",
    "                   'url':url_list,\n",
    "                   \"original title\":title_list})\n",
    "\n",
    "# insert url doman\n",
    "df_result.insert(loc=df_result.columns.get_loc('url'), column='domain', \n",
    "                 value=df_result['url'].str.split('/').str[2])\n",
    "\n",
    "# translate title into english\n",
    "df_result['english title'] = StringOperation.word_translation(df_result['original title'], src_lang='id', dest_lang='en')\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11b6e5-3d1c-40d3-b752-fd7c72d7cf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  00 HOURS 14 MINUTES 36 SECONDS VISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.loc[216, 'english title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcafe2db-c445-4d0c-8bf6-c5e5d5ac28f2",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab55eedd-9827-45d2-b147-60be64628a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "file_path = os.path.join('dataset', \"scrapping_result.csv\")\n",
    "df_result.to_csv(file_path, sep=',', index=False, quoting=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7a8bbf8280d739911ee0fb92c2a18e877feb5454e3bd0217f9b485bdc87c128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
